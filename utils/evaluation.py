import numpy as np
from collections import defaultdict

def evaluate_scheduling_result(env):
    """
    åŸºäºç¯å¢ƒæœ€ç»ˆçŠ¶æ€è¯„ä¼°è°ƒåº¦ç»“æœ

    Args:
        env: è®­ç»ƒåçš„USVè°ƒåº¦ç¯å¢ƒ

    Returns:
        dict: åŒ…å«å„ç§è¯„ä¼°æŒ‡æ ‡çš„å­—å…¸
    """
    if not env.scheduled_tasks:
        return {
            'total_makespan': float('inf'),
            'avg_completion_time': float('inf'),
            'task_completion_rate': 0.0,
            'usv_utilization': {},
            'load_balance_std': float('inf'),
            'efficiency_score': 0.0
        }

    # 1. åŸºæœ¬æ—¶é—´æŒ‡æ ‡
    completed_times = env.makespan_batch[env.scheduled_tasks]
    total_makespan = np.max(completed_times)
    avg_completion_time = np.mean(completed_times)

    # 2. ä»»åŠ¡å®Œæˆç‡
    task_completion_rate = len(env.scheduled_tasks) / env.num_tasks

    # 3. USVåˆ©ç”¨ç‡åˆ†æ
    usv_utilization = {}
    usv_task_counts = defaultdict(int)
    usv_work_times = defaultdict(float)

    # åˆ†ææ¯ä¸ªUSVçš„å·¥ä½œè´Ÿè½½
    for task_idx in env.scheduled_tasks:
        usv_idx = env.task_assignment[task_idx]
        if usv_idx != -1:
            usv_task_counts[usv_idx] += 1
            if task_idx in env.task_schedule_details:
                details = env.task_schedule_details[task_idx]
                work_time = details['travel_time'] + details['processing_time']
                usv_work_times[usv_idx] += work_time

    # è®¡ç®—æ¯ä¸ªUSVçš„åˆ©ç”¨ç‡
    for usv_idx in range(env.num_usvs):
        total_work_time = usv_work_times.get(usv_idx, 0)
        utilization_rate = total_work_time / total_makespan if total_makespan > 0 else 0
        usv_utilization[f'USV_{usv_idx}'] = {
            'task_count': usv_task_counts.get(usv_idx, 0),
            'work_time': total_work_time,
            'utilization_rate': utilization_rate
        }

    # 4. è´Ÿè½½å‡è¡¡åº¦ï¼ˆæ ‡å‡†å·®è¶Šå°è¶Šå¥½ï¼‰
    task_counts = [usv_task_counts.get(i, 0) for i in range(env.num_usvs)]
    work_times = [usv_work_times.get(i, 0) for i in range(env.num_usvs)]

    load_balance_std = {
        'task_count_std': np.std(task_counts),
        'work_time_std': np.std(work_times)
    }

    # 5. æ•ˆç‡å¾—åˆ†ï¼ˆç»¼åˆæŒ‡æ ‡ï¼‰
    # è€ƒè™‘å®Œæˆç‡ã€æ—¶é—´æ•ˆç‡ã€è´Ÿè½½å‡è¡¡
    time_efficiency = 1.0 / (1.0 + total_makespan / 1000.0)  # å½’ä¸€åŒ–æ—¶é—´æ•ˆç‡
    balance_score = 1.0 / (1.0 + np.std(work_times))  # è´Ÿè½½å‡è¡¡å¾—åˆ†
    efficiency_score = task_completion_rate * 0.5 + time_efficiency * 0.3 + balance_score * 0.2

    return {
        'total_makespan': total_makespan,
        'avg_completion_time': avg_completion_time,
        'task_completion_rate': task_completion_rate,
        'usv_utilization': usv_utilization,
        'load_balance_std': load_balance_std,
        'efficiency_score': efficiency_score,
        'detailed_metrics': {
            'min_completion_time': np.min(completed_times),
            'max_completion_time': np.max(completed_times),
            'completion_time_std': np.std(completed_times),
            'total_travel_distance': _calculate_total_travel_distance(env),
            'avg_usv_battery_remaining': np.mean(env.usv_batteries)
        }
    }


def _calculate_total_travel_distance(env):
    """è®¡ç®—æ€»æ—…è¡Œè·ç¦»"""
    total_distance = 0.0

    for task_idx in env.scheduled_tasks:
        if task_idx in env.task_schedule_details:
            details = env.task_schedule_details[task_idx]
            usv_idx = details['usv_idx']
            travel_time = details['travel_time']
            # è·ç¦» = æ—…è¡Œæ—¶é—´ * é€Ÿåº¦
            distance = travel_time * env.usv_speeds[usv_idx]
            total_distance += distance

    return total_distance


def compare_scheduling_methods(results_dict):
    """
    æ¯”è¾ƒä¸åŒè°ƒåº¦æ–¹æ³•çš„æ€§èƒ½

    Args:
        results_dict: {method_name: evaluation_result, ...}

    Returns:
        dict: æ’åºåçš„æ¯”è¾ƒç»“æœ
    """
    if not results_dict:
        return {}

    # å®šä¹‰æ¯”è¾ƒæŒ‡æ ‡ï¼ˆè¶Šå°è¶Šå¥½çš„æŒ‡æ ‡ç”¨è´Ÿå·ï¼‰
    comparison_metrics = {
        'total_makespan': 'min',  # è¶Šå°è¶Šå¥½
        'task_completion_rate': 'max',  # è¶Šå¤§è¶Šå¥½
        'efficiency_score': 'max',  # è¶Šå¤§è¶Šå¥½
        'load_balance_std.work_time_std': 'min'  # è¶Šå°è¶Šå¥½
    }

    rankings = {}

    for metric, direction in comparison_metrics.items():
        method_scores = []

        for method_name, result in results_dict.items():
            # å¤„ç†åµŒå¥—æŒ‡æ ‡
            if '.' in metric:
                keys = metric.split('.')
                value = result
                for key in keys:
                    value = value.get(key, float('inf') if direction == 'min' else 0)
            else:
                value = result.get(metric, float('inf') if direction == 'min' else 0)

            method_scores.append((method_name, value))

        # æ’åº
        reverse = (direction == 'max')
        method_scores.sort(key=lambda x: x[1], reverse=reverse)
        rankings[metric] = method_scores

    return rankings


def print_evaluation_report(evaluation_result, method_name="Current Method"):
    """æ‰“å°è¯¦ç»†çš„è¯„ä¼°æŠ¥å‘Š"""
    print(f"\n{'=' * 60}")
    print(f"è°ƒåº¦æ–¹æ³•è¯„ä¼°æŠ¥å‘Š: {method_name}")
    print(f"{'=' * 60}")

    print(f"\nğŸ“Š åŸºæœ¬æ€§èƒ½æŒ‡æ ‡:")
    print(f"  æ€»å®Œæˆæ—¶é—´ (Makespan): {evaluation_result['total_makespan']:.2f}")
    print(f"  å¹³å‡å®Œæˆæ—¶é—´: {evaluation_result['avg_completion_time']:.2f}")
    print(f"  ä»»åŠ¡å®Œæˆç‡: {evaluation_result['task_completion_rate']:.1%}")
    print(f"  æ•ˆç‡å¾—åˆ†: {evaluation_result['efficiency_score']:.3f}")

    print(f"\nğŸš¢ USVåˆ©ç”¨ç‡åˆ†æ:")
    for usv_name, util_data in evaluation_result['usv_utilization'].items():
        print(f"  {usv_name}:")
        print(f"    ä»»åŠ¡æ•°é‡: {util_data['task_count']}")
        print(f"    å·¥ä½œæ—¶é—´: {util_data['work_time']:.2f}")
        print(f"    åˆ©ç”¨ç‡: {util_data['utilization_rate']:.1%}")

    print(f"\nâš–ï¸ è´Ÿè½½å‡è¡¡:")
    balance = evaluation_result['load_balance_std']
    print(f"  ä»»åŠ¡æ•°é‡æ ‡å‡†å·®: {balance['task_count_std']:.2f}")
    print(f"  å·¥ä½œæ—¶é—´æ ‡å‡†å·®: {balance['work_time_std']:.2f}")

    print(f"\nğŸ“ˆ è¯¦ç»†æŒ‡æ ‡:")
    details = evaluation_result['detailed_metrics']
    print(f"  æœ€çŸ­å®Œæˆæ—¶é—´: {details['min_completion_time']:.2f}")
    print(f"  æœ€é•¿å®Œæˆæ—¶é—´: {details['max_completion_time']:.2f}")
    print(f"  å®Œæˆæ—¶é—´æ ‡å‡†å·®: {details['completion_time_std']:.2f}")
    print(f"  æ€»æ—…è¡Œè·ç¦»: {details['total_travel_distance']:.2f}")
    print(f"  å¹³å‡å‰©ä½™ç”µé‡: {details['avg_usv_battery_remaining']:.1f}")


# ä½¿ç”¨ç¤ºä¾‹
def evaluate_trained_model(env_after_episode):
    """
    è¯„ä¼°è®­ç»ƒåçš„æ¨¡å‹æ€§èƒ½
    åœ¨è®­ç»ƒè„šæœ¬çš„episodeç»“æŸåè°ƒç”¨
    """
    result = evaluate_scheduling_result(env_after_episode)
    print_evaluation_report(result, "PPO Trained Model")
    return result
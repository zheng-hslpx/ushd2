import os
import time
import random
import torch
import numpy as np
from tqdm import tqdm
from torch.utils.tensorboard import SummaryWriter
from env.usv_env import USVSchedulingEnv
from env.state_representation import build_heterogeneous_graph, calculate_usv_task_distances
from graph.hgnn import StableUSVHeteroGNN
from PPO_model import PPO, Memory
from utils.evaluation import evaluate_scheduling_result, print_evaluation_report
import visdom
import utils.data_generator as data_generator
import matplotlib.pyplot as plt
import matplotlib.cm as cm
import matplotlib.colors as mcolors
import logging

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[logging.StreamHandler()]
)

# ========== æ”¹è¿›çš„è®­ç»ƒé…ç½® ==========

# ç¯å¢ƒå‚æ•°
num_usvs = 3
num_tasks = 30
num_instances = 100  # å¢åŠ è®­ç»ƒæ•°æ®å¤šæ ·æ€§

# è®­ç»ƒå‚æ•°é…ç½®
TRAINING_CONFIG = {
    'max_episodes': 100,  # ç»§ç»­è®­ç»ƒ
    'max_steps_per_episode': num_tasks,
    'early_stop_patience': 300,
    'seed': 42,
    'eta': 7,  # å¢åŠ å›¾è¿æ¥åº¦
    'warmup_episodes': 200,
    'eval_frequency': 50,
    'batch_episodes': 15,  # å¢åŠ æ‰¹æ¬¡å¤§å°
    'save_frequency': 100,

    # æ–°å¢ï¼šè¯¾ç¨‹å­¦ä¹ å‚æ•°
    'curriculum_stages': [
        {'episodes': 500, 'focus': 'balance'},  # é˜¶æ®µ1ï¼šé‡è§†å‡è¡¡
        {'episodes': 1000, 'focus': 'mixed'},   # é˜¶æ®µ2ï¼šå¹³è¡¡
        {'episodes': 1500, 'focus': 'makespan'} # é˜¶æ®µ3ï¼šé‡è§†æ•ˆç‡
    ]
}

# HGNNå‚æ•°é…ç½®
HGNN_CONFIG = {
    'hidden_dim': 256,
    'n_heads': 8,
    'num_layers': 3,
    'dropout': 0.1,  # é™ä½dropout
}

# PPOå‚æ•°é…ç½®ï¼ˆæ”¹è¿›ç‰ˆï¼‰
PPO_CONFIG = {
    'lr_actor': 2e-4,  # ç¨å¾®é™ä½å­¦ä¹ ç‡
    'lr_critic': 8e-4,
    'gamma': 0.995,  # å¢åŠ è¿œè§
    'eps_clip': 0.15,  # æ›´ä¿å®ˆçš„æ›´æ–°
    'K_epochs': 12,
    'entropy_coef': 0.008,  # é™ä½æ¢ç´¢
    'value_coef': 1.2,
    'gae_lambda': 0.97,
}


def setup_seed(seed):
    """è®¾ç½®éšæœºç§å­ç¡®ä¿ç»“æœå¯å¤ç°"""
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    np.random.seed(seed)
    random.seed(seed)
    torch.backends.cudnn.deterministic = True


def adaptive_exploration(action, action_mask, episode, usv_completion_times):
    """åŸºäºUSVå®Œæˆæ—¶é—´çš„è‡ªé€‚åº”æ¢ç´¢"""
    explore_prob = max(0.02, 0.2 * (0.997 ** episode))

    if np.random.random() < explore_prob:
        valid_actions = np.where(action_mask)[0]
        if len(valid_actions) == 0:
            return action

        # åŸºäºUSVå½“å‰å®Œæˆæ—¶é—´åˆ†é…æ¦‚ç‡
        action_probs = np.ones(len(valid_actions))

        for i, valid_action in enumerate(valid_actions):
            usv_idx = valid_action // 30
            # å®Œæˆæ—¶é—´è¶Šæ—©çš„USVï¼Œè¢«é€‰æ‹©æ¦‚ç‡è¶Šé«˜
            completion_time_ranks = np.argsort(usv_completion_times)
            rank = np.where(completion_time_ranks == usv_idx)[0][0]
            action_probs[i] = 1.0 / (1.0 + rank)

        action_probs = action_probs / np.sum(action_probs)
        return np.random.choice(valid_actions, p=action_probs)

    return action


def collect_batch_episodes(env, ppo, instances, batch_size, device, config, episode_num):
    """
    æ‰¹é‡æ”¶é›†å¤šä¸ªepisodeçš„ç»éªŒ
    """
    batch_memory = Memory()
    batch_rewards = []
    batch_makespans = []
    batch_balances = []

    for _ in range(batch_size):
        # éšæœºé€‰æ‹©ä¸€ä¸ªå®ä¾‹
        instance_idx = np.random.randint(len(instances))
        tasks, usvs = instances[instance_idx]

        # é‡ç½®ç¯å¢ƒ
        state = env.reset_with_instances(tasks, usvs)

        episode_reward = 0
        steps = 0
        done = False

        # è·Ÿè¸ªUSVä»»åŠ¡åˆ†é…
        usv_task_counts = np.zeros(env.num_usvs)

        while not done and steps < config['max_steps_per_episode']:
            # æ„å»ºå›¾
            usv_feats = state['usv_features']
            task_feats = state['task_features']
            distances = calculate_usv_task_distances(usv_feats[:, :2], task_feats[:, :2])
            graph = build_heterogeneous_graph(
                usv_feats, task_feats, distances,
                eta=config['eta'], device=device
            )
            graph.action_mask = state['action_mask']

            # é€‰æ‹©åŠ¨ä½œ
            action, log_prob, state_value = ppo.select_action(graph)

            # æ™ºèƒ½æ¢ç´¢ï¼ˆè€ƒè™‘è´Ÿè½½å‡è¡¡ï¼‰
            action = adaptive_exploration(
                action, state['action_mask'],
                episode_num, usv_task_counts
            )

            # æ‰§è¡ŒåŠ¨ä½œ
            next_state, reward, done, info = env.step(action)

            # æ›´æ–°USVä»»åŠ¡è®¡æ•°
            usv_idx = action // num_tasks
            usv_task_counts[usv_idx] += 1

            # å­˜å‚¨ç»éªŒ
            batch_memory.states.append(graph)
            batch_memory.actions.append(action)
            batch_memory.logprobs.append(log_prob)
            batch_memory.rewards.append(reward)
            batch_memory.is_terminals.append(done)
            batch_memory.state_values.append(state_value)

            state = next_state
            episode_reward += reward
            steps += 1

        # è®¡ç®—è´Ÿè½½å‡è¡¡åº¦
        final_task_counts = np.bincount(
            env.task_assignment[env.task_assignment != -1],
            minlength=env.num_usvs
        )
        balance_std = np.std(final_task_counts)

        batch_rewards.append(episode_reward)
        batch_makespans.append(info.get('final_makespan', float('inf')))
        batch_balances.append(balance_std)

    return batch_memory, batch_rewards, batch_makespans, batch_balances


def evaluate_model(ppo, env, test_instances, device, config, episode):
    """
    è¯„ä¼°æ¨¡å‹æ€§èƒ½
    """
    total_rewards = []
    total_makespans = []
    load_balances = []

    ppo.policy_old.eval()  # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼

    with torch.no_grad():
        for tasks, usvs in test_instances[:10]:  # è¯„ä¼°10ä¸ªå®ä¾‹
            state = env.reset_with_instances(tasks, usvs)

            episode_reward = 0
            done = False
            steps = 0

            while not done and steps < config['max_steps_per_episode']:
                # æ„å»ºå›¾
                usv_feats = state['usv_features']
                task_feats = state['task_features']
                distances = calculate_usv_task_distances(usv_feats[:, :2], task_feats[:, :2])
                graph = build_heterogeneous_graph(
                    usv_feats, task_feats, distances,
                    eta=config['eta'], device=device
                )
                graph.action_mask = state['action_mask']

                # é€‰æ‹©åŠ¨ä½œ
                action, log_prob, state_value = ppo.select_action(graph)

                # è·å–USVå®Œæˆæ—¶é—´ç”¨äºæ¢ç´¢
                usv_completion_times = env.usv_next_available_time.copy()

                # æ™ºèƒ½æ¢ç´¢ï¼ˆåŸºäºUSVå®Œæˆæ—¶é—´ï¼‰
                action = adaptive_exploration(
                    action, state['action_mask'],
                    episode, usv_completion_times
                )

                # æ‰§è¡ŒåŠ¨ä½œ
                next_state, reward, done, info = env.step(action)

            # è®¡ç®—è´Ÿè½½å‡è¡¡
            task_counts = np.bincount(
                env.task_assignment[env.task_assignment != -1],
                minlength=env.num_usvs
            )

            total_rewards.append(episode_reward)
            total_makespans.append(info.get('final_makespan', float('inf')))
            load_balances.append(np.std(task_counts))

    ppo.policy_old.train()  # æ¢å¤è®­ç»ƒæ¨¡å¼

    return np.mean(total_rewards), np.mean(total_makespans), np.mean(load_balances)


def generate_gantt_chart(env, save_path=None):
    """ç”Ÿæˆå¹¶æ˜¾ç¤ºç”˜ç‰¹å›¾"""
    if not env.scheduled_tasks:
        print("è­¦å‘Šï¼šæ²¡æœ‰å·²è°ƒåº¦çš„ä»»åŠ¡ï¼Œæ— æ³•ç”Ÿæˆç”˜ç‰¹å›¾ã€‚")
        return

    fig, ax = plt.subplots(figsize=(15, 8))

    # ä¸ºæ¯ä¸ª USV ç”Ÿæˆå”¯ä¸€é¢œè‰²
    num_usvs = env.num_usvs
    hues = np.linspace(0, 1, num_usvs, endpoint=False)
    usv_colors_list = [mcolors.hsv_to_rgb((h, 0.8, 0.8)) for h in hues]

    # ç”¨äºå­˜å‚¨æ¯ä¸ª USV çš„ä»»åŠ¡åŠå…¶æ—¶é—´ä¿¡æ¯
    usv_task_data = {i: [] for i in range(env.num_usvs)}

    # å¡«å…… usv_task_data
    for task_idx in env.scheduled_tasks:
        if task_idx in env.task_schedule_details:
            details = env.task_schedule_details[task_idx]
            usv_idx = details['usv_idx']
            usv_task_data[usv_idx].append(details)

    # å¯¹æ¯ä¸ª USV ä¸Šçš„ä»»åŠ¡æŒ‰å¤„ç†å¼€å§‹æ—¶é—´æ’åº
    for usv_idx in usv_task_data:
        if usv_task_data[usv_idx]:
            usv_task_data[usv_idx].sort(key=lambda x: x['processing_start_time'])

    # ç»˜åˆ¶æ¯ä¸ªä»»åŠ¡çš„æ¡å½¢å›¾
    y_labels = []
    y_positions = []
    y_spacing = 1.5
    bar_height = 0.4

    for usv_idx, tasks_data in usv_task_data.items():
        y_pos = usv_idx * y_spacing
        y_labels.append(f'USV {usv_idx}')
        y_positions.append(y_pos)

        if not tasks_data:
            continue

        current_usv_color = usv_colors_list[usv_idx]

        for task_data in tasks_data:
            task_idx = task_data['task_idx']
            processing_start_time = task_data['processing_start_time']
            processing_time = task_data['processing_time']
            travel_start_time = task_data['travel_start_time']
            travel_time = task_data['travel_time']

            # ç»˜åˆ¶èˆªè¡Œæ—¶é—´æ¡
            if travel_time > 0:
                ax.barh(y_pos, travel_time, left=travel_start_time, height=bar_height,
                        color='gray', alpha=0.5)

            # ç»˜åˆ¶å¤„ç†æ—¶é—´æ¡
            ax.barh(y_pos, processing_time, left=processing_start_time, height=bar_height,
                    color=current_usv_color)

            # æ·»åŠ ä»»åŠ¡ç¼–å·
            ax.text(processing_start_time + processing_time / 2, y_pos, f'{task_idx}',
                    ha='center', va='center', fontsize=9, color='black', weight='bold')

    # è®¾ç½®åæ ‡è½´
    ax.set_yticks(y_positions)
    ax.set_yticklabels(y_labels)
    ax.set_xlabel('Time')
    ax.set_ylabel('USV')
    ax.set_title('Gantt Chart of USV Task Scheduling')

    # æ·»åŠ å›¾ä¾‹
    from matplotlib.patches import Patch
    legend_elements = [Patch(facecolor='gray', alpha=0.5, label='Navigation')]
    ax.legend(handles=legend_elements, loc='upper right')

    # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯
    task_counts = np.bincount(
        env.task_assignment[env.task_assignment != -1],
        minlength=env.num_usvs
    )
    stats_text = f"Task Distribution: {task_counts} | Std: {np.std(task_counts):.2f}"
    ax.text(0.02, 0.98, stats_text, transform=ax.transAxes,
            fontsize=10, verticalalignment='top',
            bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))

    fig.tight_layout()

    if save_path:
        plt.savefig(save_path, dpi=150, bbox_inches='tight')
        print(f"ç”˜ç‰¹å›¾å·²ä¿å­˜åˆ°: {save_path}")

    plt.show()


def main():
    """ä¸»è®­ç»ƒå‡½æ•°"""
    # è®¾ç½®éšæœºç§å­
    setup_seed(TRAINING_CONFIG['seed'])

    # è®¾å¤‡é…ç½®
    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
    print(f"Using device: {device}")

    # åˆ›å»ºç›®å½•
    model_dir = "model"
    log_dir = "logs"
    os.makedirs(model_dir, exist_ok=True)
    os.makedirs(log_dir, exist_ok=True)

    # åˆå§‹åŒ–ç¯å¢ƒ
    print("åˆå§‹åŒ–ç¯å¢ƒå’Œæ¨¡å‹...")
    env = USVSchedulingEnv(num_usvs=num_usvs, num_tasks=num_tasks)

    # è·å–ç‰¹å¾ç»´åº¦
    temp_state = env.reset()
    usv_feat_dim = temp_state['usv_features'].shape[1]
    task_feat_dim = temp_state['task_features'].shape[1]
    action_dim = num_usvs * num_tasks

    print(f"ç‰¹å¾ç»´åº¦ - USV: {usv_feat_dim}, Task: {task_feat_dim}, Action: {action_dim}")

    # åˆå§‹åŒ–HGNNæ¨¡å‹
    hgnn = StableUSVHeteroGNN(
        usv_feat_dim=usv_feat_dim,
        task_feat_dim=task_feat_dim,
        hidden_dim=HGNN_CONFIG['hidden_dim'],
        n_heads=HGNN_CONFIG['n_heads'],
        num_layers=HGNN_CONFIG['num_layers'],
        dropout=HGNN_CONFIG['dropout']
    ).to(device)

    # åˆå§‹åŒ–PPO
    # æ³¨æ„ï¼šå¦‚æœæ‚¨çš„PPOç±»æ”¯æŒåˆ†ç¦»çš„å­¦ä¹ ç‡ï¼Œè¯·ä½¿ç”¨ä»¥ä¸‹å‚æ•°
    # å¦‚æœä¸æ”¯æŒï¼Œè¯·ä½¿ç”¨å•ä¸€å­¦ä¹ ç‡
    try:
        ppo = PPO(
            hgnn=hgnn,
            action_dim=action_dim,
            lr_actor=PPO_CONFIG['lr_actor'],
            lr_critic=PPO_CONFIG['lr_critic'],
            gamma=PPO_CONFIG['gamma'],
            eps_clip=PPO_CONFIG['eps_clip'],
            K_epochs=PPO_CONFIG['K_epochs'],
            device=device,
            entropy_coef=PPO_CONFIG['entropy_coef'],
            value_coef=PPO_CONFIG['value_coef'],
            gae_lambda=PPO_CONFIG.get('gae_lambda', 0.95)
        )
    except TypeError:
        # å¦‚æœä¸æ”¯æŒåˆ†ç¦»å­¦ä¹ ç‡ï¼Œä½¿ç”¨å•ä¸€å­¦ä¹ ç‡
        print("ä½¿ç”¨å•ä¸€å­¦ä¹ ç‡é…ç½®")
        ppo = PPO(
            hgnn=hgnn,
            action_dim=action_dim,
            lr=PPO_CONFIG['lr_actor'],
            gamma=PPO_CONFIG['gamma'],
            eps_clip=PPO_CONFIG['eps_clip'],
            K_epochs=PPO_CONFIG['K_epochs'],
            device=device,
            entropy_coef=PPO_CONFIG['entropy_coef'],
            value_coef=PPO_CONFIG['value_coef']
        )

    # åˆå§‹åŒ–TensorBoard
    writer = SummaryWriter(f"runs/usv_scheduling_{time.strftime('%Y%m%d_%H%M%S')}")

    # åˆå§‹åŒ–Visdomï¼ˆå¯é€‰ï¼‰
    vis = None
    try:
        vis = visdom.Visdom()
        if vis.check_connection():
            print("Visdomè¿æ¥æˆåŠŸ")
            # åˆå§‹åŒ–å¯è§†åŒ–çª—å£
            vis_windows = {
                'reward': vis.line(Y=torch.zeros((1)).cpu(), X=torch.zeros((1)).cpu(),
                                   opts=dict(xlabel='Episode', ylabel='Reward', title='Training Reward')),
                'makespan': vis.line(Y=torch.zeros((1)).cpu(), X=torch.zeros((1)).cpu(),
                                     opts=dict(xlabel='Episode', ylabel='Makespan', title='Training Makespan')),
                'balance': vis.line(Y=torch.zeros((1)).cpu(), X=torch.zeros((1)).cpu(),
                                    opts=dict(xlabel='Episode', ylabel='Std', title='Load Balance')),
                'losses': vis.line(Y=torch.zeros((1, 2)).cpu(), X=torch.zeros((1)).cpu(),
                                   opts=dict(xlabel='Episode', ylabel='Loss', title='Losses',
                                             legend=['Policy', 'Value']))
            }
        else:
            print("Visdomæœªè¿æ¥ï¼Œå°†è·³è¿‡å¯è§†åŒ–")
            vis = None
    except Exception as e:
        print(f"Visdomåˆå§‹åŒ–å¤±è´¥: {e}")
        vis = None

    # ç”Ÿæˆè®­ç»ƒæ•°æ®
    print(f"ç”Ÿæˆ{num_instances}ä¸ªè®­ç»ƒå®ä¾‹...")
    train_instances = data_generator.generate_batch_instances(
        num_instances=num_instances,
        fixed_tasks=num_tasks,
        fixed_usvs=num_usvs
    )

    # è®­ç»ƒç»Ÿè®¡
    all_rewards = []
    all_makespans = []
    all_balances = []
    best_avg_reward = -float('inf')
    best_avg_makespan = float('inf')
    best_balance = float('inf')
    no_improve_count = 0

    print("\n" + "=" * 60)
    print("ğŸš€ å¼€å§‹è®­ç»ƒ")
    print("=" * 60)

    episode = 0
    pbar = tqdm(total=TRAINING_CONFIG['max_episodes'], desc="è®­ç»ƒè¿›åº¦")

    while episode < TRAINING_CONFIG['max_episodes']:
        # æ‰¹é‡æ”¶é›†ç»éªŒ
        batch_memory, batch_rewards, batch_makespans, batch_balances = collect_batch_episodes(
            env, ppo, train_instances,
            TRAINING_CONFIG['batch_episodes'],
            device, TRAINING_CONFIG, episode
        )
        episode += TRAINING_CONFIG['batch_episodes']

        # æ›´æ–°PPO
        if len(batch_memory.states) > 0:
            # æ ¹æ®æ‚¨çš„PPOå®ç°é€‰æ‹©æ­£ç¡®çš„è¿”å›å€¼
            losses = ppo.update(batch_memory)
            if isinstance(losses, tuple) and len(losses) >= 2:
                policy_loss, value_loss = losses[0], losses[1]
                entropy_loss = losses[2] if len(losses) > 2 else 0
            else:
                policy_loss = value_loss = entropy_loss = 0
        else:
            policy_loss = value_loss = entropy_loss = 0

        # æ›´æ–°ç»Ÿè®¡
        all_rewards.extend(batch_rewards)
        all_makespans.extend(batch_makespans)
        all_balances.extend(batch_balances)
        episode += TRAINING_CONFIG['batch_episodes']

        # é™åˆ¶å†å²è®°å½•é•¿åº¦
        if len(all_rewards) > 1000:
            all_rewards = all_rewards[-1000:]
            all_makespans = all_makespans[-1000:]
            all_balances = all_balances[-1000:]

        # è®¡ç®—å¹³å‡æŒ‡æ ‡
        avg_reward = np.mean(batch_rewards)
        avg_makespan = np.mean(batch_makespans)
        avg_balance = np.mean(batch_balances)

        # è®°å½•åˆ°TensorBoard
        writer.add_scalar("Batch/AvgReward", avg_reward, episode)
        writer.add_scalar("Batch/AvgMakespan", avg_makespan, episode)
        writer.add_scalar("Batch/AvgBalance", avg_balance, episode)
        writer.add_scalar("Loss/Policy", policy_loss, episode)
        writer.add_scalar("Loss/Value", value_loss, episode)
        writer.add_scalar("Loss/Entropy", entropy_loss, episode)

        # æ›´æ–°Visdom
        if vis and 'reward' in vis_windows:
            try:
                vis.line(Y=torch.tensor([avg_reward]).cpu(), X=torch.tensor([episode]).cpu(),
                         win=vis_windows['reward'], update='append')
                vis.line(Y=torch.tensor([avg_makespan]).cpu(), X=torch.tensor([episode]).cpu(),
                         win=vis_windows['makespan'], update='append')
                vis.line(Y=torch.tensor([avg_balance]).cpu(), X=torch.tensor([episode]).cpu(),
                         win=vis_windows['balance'], update='append')
                vis.line(Y=torch.tensor([[policy_loss, value_loss]]).cpu(),
                         X=torch.tensor([episode]).cpu(),
                         win=vis_windows['losses'], update='append')
            except:
                pass

        # æ›´æ–°è¿›åº¦æ¡
        pbar.update(TRAINING_CONFIG['batch_episodes'])
        pbar.set_postfix({
            'R': f"{avg_reward:.1f}",
            'M': f"{avg_makespan:.1f}",
            'B': f"{avg_balance:.2f}",
            'PL': f"{policy_loss:.3f}",
            'VL': f"{value_loss:.3f}"
        })

        # å®šæœŸè¯„ä¼°
        if episode % TRAINING_CONFIG['eval_frequency'] == 0 and episode > 0:
            eval_reward, eval_makespan, eval_balance = evaluate_model(
                ppo, env, train_instances, device, TRAINING_CONFIG
            )

            print(f"\nğŸ“Š Episode {episode} è¯„ä¼°ç»“æœ:")
            print(f"  å¹³å‡å¥–åŠ±: {eval_reward:.2f}")
            print(f"  å¹³å‡Makespan: {eval_makespan:.2f}")
            print(f"  è´Ÿè½½å‡è¡¡åº¦(std): {eval_balance:.2f}")

            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if eval_reward > best_avg_reward and eval_balance < 3.0:  # ç¡®ä¿è´Ÿè½½å‡è¡¡
                best_avg_reward = eval_reward
                best_avg_makespan = eval_makespan
                best_balance = eval_balance
                no_improve_count = 0

                torch.save({
                    'model_state_dict': ppo.policy.state_dict(),
                    'optimizer_state_dict': ppo.optimizer.state_dict() if hasattr(ppo, 'optimizer') else None,
                    'episode': episode,
                    'avg_reward': best_avg_reward,
                    'avg_makespan': best_avg_makespan,
                    'balance': best_balance,
                    'config': {
                        'hgnn': HGNN_CONFIG,
                        'ppo': PPO_CONFIG,
                        'training': TRAINING_CONFIG
                    }
                }, f'{model_dir}/best_model.pt')
                print(f"  âœ… ä¿å­˜æœ€ä½³æ¨¡å‹")
            else:
                no_improve_count += 1

        # å®šæœŸä¿å­˜æ£€æŸ¥ç‚¹
        if episode % TRAINING_CONFIG['save_frequency'] == 0 and episode > 0:
            torch.save({
                'model_state_dict': ppo.policy.state_dict(),
                'optimizer_state_dict': ppo.optimizer.state_dict() if hasattr(ppo, 'optimizer') else None,
                'episode': episode,
                'all_rewards': all_rewards[-100:],
                'all_makespans': all_makespans[-100:],
                'all_balances': all_balances[-100:]
            }, f'{model_dir}/checkpoint_ep{episode}.pt')

        # æ—©åœæ£€æŸ¥
        if no_improve_count >= TRAINING_CONFIG['early_stop_patience']:
            print(f"\nâš ï¸ æ—©åœ: è¿ç»­{no_improve_count}æ¬¡è¯„ä¼°æ— æ”¹è¿›")
            break

    pbar.close()

    # è®­ç»ƒç»“æŸ
    print("\n" + "=" * 60)
    print("âœ… è®­ç»ƒå®Œæˆ!")
    print(f"æœ€ä½³å¹³å‡å¥–åŠ±: {best_avg_reward:.2f}")
    print(f"æœ€ä½³å¹³å‡Makespan: {best_avg_makespan:.2f}")
    print(f"æœ€ä½³è´Ÿè½½å‡è¡¡åº¦: {best_balance:.2f}")
    print("=" * 60)

    # ç”Ÿæˆæœ€ç»ˆç”˜ç‰¹å›¾
    print("\nç”Ÿæˆæœ€ç»ˆè°ƒåº¦ç”˜ç‰¹å›¾...")
    try:
        # åŠ è½½æœ€ä½³æ¨¡å‹
        checkpoint = torch.load(f'{model_dir}/best_model.pt')
        ppo.policy.load_state_dict(checkpoint['model_state_dict'])

        # è¿è¡Œä¸€ä¸ªå®Œæ•´çš„episode
        tasks, usvs = train_instances[0]
        state = env.reset_with_instances(tasks, usvs)
        done = False
        steps = 0

        while not done and steps < num_tasks:
            usv_feats = state['usv_features']
            task_feats = state['task_features']
            distances = calculate_usv_task_distances(usv_feats[:, :2], task_feats[:, :2])
            graph = build_heterogeneous_graph(
                usv_feats, task_feats, distances,
                eta=TRAINING_CONFIG['eta'], device=device
            )
            graph.action_mask = state['action_mask']

            with torch.no_grad():
                action, _, _ = ppo.policy_old.act(graph, device, state['action_mask'])

            state, _, done, _ = env.step(action)
            steps += 1

        # ç”Ÿæˆç”˜ç‰¹å›¾
        generate_gantt_chart(env, save_path=f'{model_dir}/final_gantt.png')

        # æ‰“å°æœ€ç»ˆè¯„ä¼°æŠ¥å‘Š
        eval_result = evaluate_scheduling_result(env)
        print_evaluation_report(eval_result, "Final PPO Model")

    except Exception as e:
        print(f"ç”Ÿæˆç”˜ç‰¹å›¾æ—¶å‡ºé”™: {e}")

    # å…³é—­èµ„æº
    writer.close()

    print("\nè®­ç»ƒè„šæœ¬æ‰§è¡Œå®Œæ¯•ï¼")


if __name__ == "__main__":
    main()

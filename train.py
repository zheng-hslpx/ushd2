import os
import time
import random
import torch
import numpy as np
from tqdm import tqdm
from torch.utils.tensorboard import SummaryWriter
from env.usv_env import USVSchedulingEnv
from env.state_representation import build_heterogeneous_graph, calculate_usv_task_distances
from graph.hgnn import StableUSVHeteroGNN
from PPO_model import PPO, Memory
from utils.evaluation import evaluate_scheduling_result, print_evaluation_report
import visdom
import utils.data_generator as data_generator
import matplotlib.pyplot as plt
import matplotlib.colors as mcolors
import logging

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[logging.StreamHandler()]
)

# ========== ä¼˜åŒ–çš„è®­ç»ƒé…ç½® ==========

# ç¯å¢ƒå‚æ•°
num_usvs = 3
num_tasks = 30
num_instances = 200  # å¢åŠ æ•°æ®å¤šæ ·æ€§

# è®­ç»ƒå‚æ•°é…ç½®ï¼ˆä¼˜åŒ–ç‰ˆï¼‰
TRAINING_CONFIG = {
    'max_episodes': 2000,  # å¢åŠ è®­ç»ƒè½®æ•°
    'max_steps_per_episode': num_tasks,
    'early_stop_patience': 500,  # å¢åŠ è€å¿ƒ
    'seed': 42,
    'eta': 5,  # é€‚åº¦çš„å›¾è¿æ¥åº¦
    'warmup_episodes': 100,  # å‡å°‘é¢„çƒ­æœŸ
    'eval_frequency': 25,  # æ›´é¢‘ç¹çš„è¯„ä¼°
    'batch_episodes': 10,  # é€‚ä¸­çš„æ‰¹æ¬¡å¤§å°
    'save_frequency': 100,

    # æ–°å¢ï¼šè‡ªé€‚åº”æ¢ç´¢ç­–ç•¥
    'exploration_decay_rate': 0.995,
    'min_exploration_rate': 0.01,
    'initial_exploration_rate': 0.3,
}

# HGNNå‚æ•°é…ç½®ï¼ˆä¼˜åŒ–ï¼‰
HGNN_CONFIG = {
    'hidden_dim': 256,
    'n_heads': 8,
    'num_layers': 3,
    'dropout': 0.05,  # é™ä½dropout
}

# PPOå‚æ•°é…ç½®ï¼ˆé’ˆå¯¹makespanä¼˜åŒ–è°ƒæ•´ï¼‰
PPO_CONFIG = {
    'lr_actor': 1e-4,  # é™ä½å­¦ä¹ ç‡ï¼Œæ›´ç¨³å®š
    'lr_critic': 5e-4,
    'gamma': 0.99,  # å¢åŠ è¿œè§
    'eps_clip': 0.2,  # æ ‡å‡†å€¼
    'K_epochs': 20,  # å¢åŠ æ›´æ–°æ¬¡æ•°  15
    'batch_episodes': 20,  # å¢åŠ æ‰¹æ¬¡å¤§å°ï¼ˆåŸæ¥æ˜¯10ï¼‰
    'entropy_coef': 0.005,  # é™ä½æ¢ç´¢å™ªå£°
    'value_coef': 1.0,
    'gae_lambda': 0.98,  # å¢åŠ ä¼˜åŠ¿ä¼°è®¡çš„å‡†ç¡®æ€§
}


def setup_seed(seed):
    """è®¾ç½®éšæœºç§å­ç¡®ä¿ç»“æœå¯å¤ç°"""
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    np.random.seed(seed)
    random.seed(seed)
    torch.backends.cudnn.deterministic = True


def makespan_aware_exploration(action, action_mask, episode, env, exploration_rate):
    """
    åŸºäºMakespançš„æ™ºèƒ½æ¢ç´¢ç­–ç•¥
    ä¼˜å…ˆé€‰æ‹©å½“å‰å®Œæˆæ—¶é—´æœ€æ—©çš„USV
    """
    if np.random.random() < exploration_rate:
        valid_actions = np.where(action_mask)[0]
        if len(valid_actions) == 0:
            return action

        # è·å–æ¯ä¸ªæœ‰æ•ˆåŠ¨ä½œå¯¹åº”çš„USV
        action_scores = np.zeros(len(valid_actions))

        for i, valid_action in enumerate(valid_actions):
            usv_idx = valid_action // 30
            # å½“å‰å®Œæˆæ—¶é—´è¶Šæ—©çš„USVï¼Œè¢«é€‰æ‹©æ¦‚ç‡è¶Šé«˜
            completion_time = env.usv_next_available_time[usv_idx]
            # ä½¿ç”¨å€’æ•°ä½œä¸ºå¾—åˆ†ï¼ˆå®Œæˆæ—¶é—´è¶Šå°ï¼Œå¾—åˆ†è¶Šé«˜ï¼‰
            action_scores[i] = 1.0 / (1.0 + completion_time)

        # å½’ä¸€åŒ–ä¸ºæ¦‚ç‡
        action_probs = action_scores / np.sum(action_scores)

        # æŒ‰æ¦‚ç‡é€‰æ‹©åŠ¨ä½œ
        return np.random.choice(valid_actions, p=action_probs)

    return action


def collect_batch_episodes(env, ppo, instances, batch_size, device, config, episode_num, exploration_rate):
    """
    æ‰¹é‡æ”¶é›†å¤šä¸ªepisodeçš„ç»éªŒ
    """
    batch_memory = Memory()
    batch_rewards = []
    batch_makespans = []
    batch_balances = []

    for _ in range(batch_size):
        # éšæœºé€‰æ‹©ä¸€ä¸ªå®ä¾‹
        instance_idx = np.random.randint(len(instances))
        tasks, usvs = instances[instance_idx]

        # é‡ç½®ç¯å¢ƒ
        state = env.reset_with_instances(tasks, usvs)

        episode_reward = 0
        steps = 0
        done = False

        # è®°å½•æ¯æ­¥çš„makespanå˜åŒ–
        makespan_trajectory = []

        while not done and steps < config['max_steps_per_episode']:
            # æ„å»ºå›¾
            usv_feats = state['usv_features']
            task_feats = state['task_features']
            distances = calculate_usv_task_distances(usv_feats[:, :2], task_feats[:, :2])
            graph = build_heterogeneous_graph(
                usv_feats, task_feats, distances,
                eta=config['eta'], device=device
            )
            graph.action_mask = state['action_mask']

            # é€‰æ‹©åŠ¨ä½œ
            action, log_prob, state_value = ppo.select_action(graph)

            # æ™ºèƒ½æ¢ç´¢ï¼ˆåŸºäºmakespanï¼‰
            action = makespan_aware_exploration(
                action, state['action_mask'],
                episode_num, env, exploration_rate
            )

            # æ‰§è¡ŒåŠ¨ä½œ
            next_state, reward, done, info = env.step(action)

            # è®°å½•makespanå˜åŒ–
            makespan_trajectory.append(info['makespan'])

            # å­˜å‚¨ç»éªŒ
            batch_memory.states.append(graph)
            batch_memory.actions.append(action)
            batch_memory.logprobs.append(log_prob)
            batch_memory.rewards.append(reward)
            batch_memory.is_terminals.append(done)
            batch_memory.state_values.append(state_value)

            state = next_state
            episode_reward += reward
            steps += 1

        # è®¡ç®—è´Ÿè½½å‡è¡¡åº¦
        if env.task_assignment is not None:
            final_task_counts = np.bincount(
                env.task_assignment[env.task_assignment != -1],
                minlength=env.num_usvs
            )
            balance_std = np.std(final_task_counts)
        else:
            balance_std = float('inf')

        batch_rewards.append(episode_reward)
        batch_makespans.append(info.get('final_makespan', float('inf')))
        batch_balances.append(balance_std)

    return batch_memory, batch_rewards, batch_makespans, batch_balances


def evaluate_model(ppo, env, test_instances, device, config, episode):
    """
    è¯„ä¼°æ¨¡å‹æ€§èƒ½
    """
    total_rewards = []
    total_makespans = []
    load_balances = []

    ppo.policy_old.eval()  # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼

    with torch.no_grad():
        for tasks, usvs in test_instances[:20]:  # è¯„ä¼°20ä¸ªå®ä¾‹
            state = env.reset_with_instances(tasks, usvs)

            episode_reward = 0
            done = False
            steps = 0

            while not done and steps < config['max_steps_per_episode']:
                # æ„å»ºå›¾
                usv_feats = state['usv_features']
                task_feats = state['task_features']
                distances = calculate_usv_task_distances(usv_feats[:, :2], task_feats[:, :2])
                graph = build_heterogeneous_graph(
                    usv_feats, task_feats, distances,
                    eta=config['eta'], device=device
                )
                graph.action_mask = state['action_mask']

                # é€‰æ‹©åŠ¨ä½œï¼ˆè¯„ä¼°æ—¶ä¸æ¢ç´¢ï¼‰
                action, _, _ = ppo.select_action(graph)

                # æ‰§è¡ŒåŠ¨ä½œ
                next_state, reward, done, info = env.step(action)

                state = next_state
                episode_reward += reward
                steps += 1

            # è®¡ç®—è´Ÿè½½å‡è¡¡
            if env.task_assignment is not None:
                task_counts = np.bincount(
                    env.task_assignment[env.task_assignment != -1],
                    minlength=env.num_usvs
                )
                load_balances.append(np.std(task_counts))

            total_rewards.append(episode_reward)
            total_makespans.append(info.get('final_makespan', float('inf')))

    ppo.policy_old.train()  # æ¢å¤è®­ç»ƒæ¨¡å¼

    return np.mean(total_rewards), np.mean(total_makespans), np.mean(load_balances)


def generate_gantt_chart(env, save_path=None):
    """ç”Ÿæˆå¹¶æ˜¾ç¤ºç”˜ç‰¹å›¾"""
    if not env.scheduled_tasks:
        print("è­¦å‘Šï¼šæ²¡æœ‰å·²è°ƒåº¦çš„ä»»åŠ¡ï¼Œæ— æ³•ç”Ÿæˆç”˜ç‰¹å›¾ã€‚")
        return

    fig, ax = plt.subplots(figsize=(15, 8))

    # ä¸ºæ¯ä¸ªUSVç”Ÿæˆå”¯ä¸€é¢œè‰²
    num_usvs = env.num_usvs
    hues = np.linspace(0, 1, num_usvs, endpoint=False)
    usv_colors_list = [mcolors.hsv_to_rgb((h, 0.8, 0.8)) for h in hues]

    # ç”¨äºå­˜å‚¨æ¯ä¸ªUSVçš„ä»»åŠ¡åŠå…¶æ—¶é—´ä¿¡æ¯
    usv_task_data = {i: [] for i in range(env.num_usvs)}

    # å¡«å……usv_task_data
    for task_idx in env.scheduled_tasks:
        if task_idx in env.task_schedule_details:
            details = env.task_schedule_details[task_idx]
            usv_idx = details['usv_idx']
            usv_task_data[usv_idx].append(details)

    # å¯¹æ¯ä¸ªUSVä¸Šçš„ä»»åŠ¡æŒ‰å¤„ç†å¼€å§‹æ—¶é—´æ’åº
    for usv_idx in usv_task_data:
        if usv_task_data[usv_idx]:
            usv_task_data[usv_idx].sort(key=lambda x: x['processing_start_time'])

    # ç»˜åˆ¶æ¯ä¸ªä»»åŠ¡çš„æ¡å½¢å›¾
    y_labels = []
    y_positions = []
    y_spacing = 1.5
    bar_height = 0.4

    for usv_idx, tasks_data in usv_task_data.items():
        y_pos = usv_idx * y_spacing
        y_labels.append(f'USV {usv_idx}')
        y_positions.append(y_pos)

        if not tasks_data:
            continue

        current_usv_color = usv_colors_list[usv_idx]

        for task_data in tasks_data:
            task_idx = task_data['task_idx']
            processing_start_time = task_data['processing_start_time']
            processing_time = task_data['processing_time']
            travel_start_time = task_data['travel_start_time']
            travel_time = task_data['travel_time']

            # ç»˜åˆ¶èˆªè¡Œæ—¶é—´æ¡
            if travel_time > 0:
                ax.barh(y_pos, travel_time, left=travel_start_time, height=bar_height,
                        color='gray', alpha=0.5)

            # ç»˜åˆ¶å¤„ç†æ—¶é—´æ¡
            ax.barh(y_pos, processing_time, left=processing_start_time, height=bar_height,
                    color=current_usv_color)

            # æ·»åŠ ä»»åŠ¡ç¼–å·
            ax.text(processing_start_time + processing_time / 2, y_pos, f'{task_idx}',
                    ha='center', va='center', fontsize=9, color='black', weight='bold')

    # è®¾ç½®åæ ‡è½´
    ax.set_yticks(y_positions)
    ax.set_yticklabels(y_labels)
    ax.set_xlabel('Time')
    ax.set_ylabel('USV')
    ax.set_title('Gantt Chart of USV Task Scheduling (Optimized)')

    # æ·»åŠ å›¾ä¾‹
    from matplotlib.patches import Patch
    legend_elements = [Patch(facecolor='gray', alpha=0.5, label='Navigation')]
    ax.legend(handles=legend_elements, loc='upper right')

    # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯
    task_counts = np.bincount(
        env.task_assignment[env.task_assignment != -1],
        minlength=env.num_usvs
    )
    makespan = env.current_makespan
    stats_text = f"Makespan: {makespan:.1f} | Task Distribution: {task_counts} | Std: {np.std(task_counts):.2f}"
    ax.text(0.02, 0.98, stats_text, transform=ax.transAxes,
            fontsize=10, verticalalignment='top',
            bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))

    fig.tight_layout()

    if save_path:
        plt.savefig(save_path, dpi=150, bbox_inches='tight')
        print(f"ç”˜ç‰¹å›¾å·²ä¿å­˜åˆ°: {save_path}")

    plt.show()


def main():
    """ä¸»è®­ç»ƒå‡½æ•°"""
    # è®¾ç½®éšæœºç§å­
    setup_seed(TRAINING_CONFIG['seed'])

    # è®¾å¤‡é…ç½®
    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
    print(f"Using device: {device}")

    # åˆ›å»ºç›®å½•
    model_dir = "model"
    log_dir = "logs"
    os.makedirs(model_dir, exist_ok=True)
    os.makedirs(log_dir, exist_ok=True)

    # åˆå§‹åŒ–ç¯å¢ƒ
    print("åˆå§‹åŒ–ç¯å¢ƒå’Œæ¨¡å‹...")
    env = USVSchedulingEnv(num_usvs=num_usvs, num_tasks=num_tasks)

    # è·å–ç‰¹å¾ç»´åº¦
    temp_state = env.reset()
    usv_feat_dim = temp_state['usv_features'].shape[1]
    task_feat_dim = temp_state['task_features'].shape[1]
    action_dim = num_usvs * num_tasks

    print(f"ç‰¹å¾ç»´åº¦ - USV: {usv_feat_dim}, Task: {task_feat_dim}, Action: {action_dim}")

    # åˆå§‹åŒ–PPO
    ppo = PPO(
        action_dim=action_dim,
        usv_feat_dim=usv_feat_dim,
        task_feat_dim=task_feat_dim,
        hidden_dim=HGNN_CONFIG['hidden_dim'],
        n_heads=HGNN_CONFIG['n_heads'],
        num_layers=HGNN_CONFIG['num_layers'],
        dropout=HGNN_CONFIG['dropout'],
        lr_actor=PPO_CONFIG['lr_actor'],
        lr_critic=PPO_CONFIG['lr_critic'],
        gamma=PPO_CONFIG['gamma'],
        eps_clip=PPO_CONFIG['eps_clip'],
        K_epochs=PPO_CONFIG['K_epochs'],
        device=device,
        entropy_coef=PPO_CONFIG['entropy_coef'],
        value_coef=PPO_CONFIG['value_coef'],
        gae_lambda=PPO_CONFIG['gae_lambda']
    )

    # åˆå§‹åŒ–TensorBoard
    writer = SummaryWriter(f"runs/usv_scheduling_makespan_{time.strftime('%Y%m%d_%H%M%S')}")

    # åˆå§‹åŒ–Visdom
    vis = None
    try:
        vis = visdom.Visdom()
        if vis.check_connection():
            print("Visdomè¿æ¥æˆåŠŸ")
            vis_windows = {
                'reward': vis.line(Y=torch.zeros((1)).cpu(), X=torch.zeros((1)).cpu(),
                                   opts=dict(xlabel='Episode', ylabel='Reward', title='Training Reward')),
                'makespan': vis.line(Y=torch.zeros((1)).cpu(), X=torch.zeros((1)).cpu(),
                                     opts=dict(xlabel='Episode', ylabel='Makespan', title='Training Makespan')),
                'balance': vis.line(Y=torch.zeros((1)).cpu(), X=torch.zeros((1)).cpu(),
                                    opts=dict(xlabel='Episode', ylabel='Std', title='Load Balance')),
                'losses': vis.line(Y=torch.zeros((1, 2)).cpu(), X=torch.zeros((1)).cpu(),
                                   opts=dict(xlabel='Episode', ylabel='Loss', title='Losses',
                                             legend=['Policy', 'Value']))
            }
        else:
            vis = None
    except:
        vis = None

    # ç”Ÿæˆè®­ç»ƒæ•°æ®
    print(f"ç”Ÿæˆ{num_instances}ä¸ªè®­ç»ƒå®ä¾‹...")
    train_instances = data_generator.generate_batch_instances(
        num_instances=num_instances,
        fixed_tasks=num_tasks,
        fixed_usvs=num_usvs
    )

    # è®­ç»ƒç»Ÿè®¡
    all_rewards = []
    all_makespans = []
    all_balances = []
    best_avg_reward = -float('inf')
    best_avg_makespan = float('inf')
    best_balance = float('inf')
    no_improve_count = 0

    # æ¢ç´¢ç‡
    exploration_rate = TRAINING_CONFIG['initial_exploration_rate']

    print("\n" + "=" * 60)
    print("ğŸš€ å¼€å§‹è®­ç»ƒ (Makespanä¼˜åŒ–ç‰ˆ)")
    print("=" * 60)

    episode = 0
    pbar = tqdm(total=TRAINING_CONFIG['max_episodes'], desc="è®­ç»ƒè¿›åº¦")

    while episode < TRAINING_CONFIG['max_episodes']:
        # æ›´æ–°æ¢ç´¢ç‡
        exploration_rate = max(
            TRAINING_CONFIG['min_exploration_rate'],
            exploration_rate * TRAINING_CONFIG['exploration_decay_rate']
        )

        # æ‰¹é‡æ”¶é›†ç»éªŒ
        batch_memory, batch_rewards, batch_makespans, batch_balances = collect_batch_episodes(
            env, ppo, train_instances,
            TRAINING_CONFIG['batch_episodes'],
            device, TRAINING_CONFIG, episode, exploration_rate
        )

        # æ›´æ–°PPO
        if len(batch_memory.states) > 0:
            losses = ppo.update(batch_memory)
            if isinstance(losses, tuple) and len(losses) >= 2:
                policy_loss, value_loss = losses[0], losses[1]
                entropy_loss = losses[2] if len(losses) > 2 else 0
            else:
                policy_loss = value_loss = entropy_loss = 0

        # æ›´æ–°ç»Ÿè®¡
        all_rewards.extend(batch_rewards)
        all_makespans.extend(batch_makespans)
        all_balances.extend(batch_balances)
        episode += TRAINING_CONFIG['batch_episodes']

        # é™åˆ¶å†å²è®°å½•é•¿åº¦
        if len(all_rewards) > 1000:
            all_rewards = all_rewards[-1000:]
            all_makespans = all_makespans[-1000:]
            all_balances = all_balances[-1000:]

        # è®¡ç®—å¹³å‡æŒ‡æ ‡
        avg_reward = np.mean(batch_rewards)
        avg_makespan = np.mean(batch_makespans)
        avg_balance = np.mean(batch_balances)

        # è®°å½•åˆ°TensorBoard
        writer.add_scalar("Batch/AvgReward", avg_reward, episode)
        writer.add_scalar("Batch/AvgMakespan", avg_makespan, episode)
        writer.add_scalar("Batch/AvgBalance", avg_balance, episode)
        writer.add_scalar("Loss/Policy", policy_loss, episode)
        writer.add_scalar("Loss/Value", value_loss, episode)
        writer.add_scalar("Loss/Entropy", entropy_loss, episode)
        writer.add_scalar("Exploration/Rate", exploration_rate, episode)

        # æ›´æ–°Visdom
        if vis and 'reward' in vis_windows:
            try:
                vis.line(Y=torch.tensor([avg_reward]).cpu(), X=torch.tensor([episode]).cpu(),
                         win=vis_windows['reward'], update='append')
                vis.line(Y=torch.tensor([avg_makespan]).cpu(), X=torch.tensor([episode]).cpu(),
                         win=vis_windows['makespan'], update='append')
                vis.line(Y=torch.tensor([avg_balance]).cpu(), X=torch.tensor([episode]).cpu(),
                         win=vis_windows['balance'], update='append')
                vis.line(Y=torch.tensor([[policy_loss, value_loss]]).cpu(),
                         X=torch.tensor([episode]).cpu(),
                         win=vis_windows['losses'], update='append')
            except:
                pass

        # æ›´æ–°è¿›åº¦æ¡
        pbar.update(TRAINING_CONFIG['batch_episodes'])
        pbar.set_postfix({
            'R': f"{avg_reward:.1f}",
            'M': f"{avg_makespan:.1f}",
            'B': f"{avg_balance:.2f}",
            'Exp': f"{exploration_rate:.3f}"
        })

        # å®šæœŸè¯„ä¼°
        if episode % TRAINING_CONFIG['eval_frequency'] == 0 and episode > 0:
            eval_reward, eval_makespan, eval_balance = evaluate_model(
                ppo, env, train_instances, device, TRAINING_CONFIG, episode
            )

            print(f"\nğŸ“Š Episode {episode} è¯„ä¼°ç»“æœ:")
            print(f"  å¹³å‡å¥–åŠ±: {eval_reward:.2f}")
            print(f"  å¹³å‡Makespan: {eval_makespan:.2f}")
            print(f"  è´Ÿè½½å‡è¡¡åº¦(std): {eval_balance:.2f}")
            print(f"  æ¢ç´¢ç‡: {exploration_rate:.3f}")

            # ä¿å­˜æœ€ä½³æ¨¡å‹ï¼ˆä¸»è¦å…³æ³¨makespanï¼‰
            if eval_makespan < best_avg_makespan:
                best_avg_reward = eval_reward
                best_avg_makespan = eval_makespan
                best_balance = eval_balance
                no_improve_count = 0

                torch.save({
                    'model_state_dict': ppo.policy.state_dict(),
                    'episode': episode,
                    'avg_reward': best_avg_reward,
                    'avg_makespan': best_avg_makespan,
                    'balance': best_balance,
                    'config': {
                        'hgnn': HGNN_CONFIG,
                        'ppo': PPO_CONFIG,
                        'training': TRAINING_CONFIG
                    }
                }, f'{model_dir}/best_model.pt')
                print(f"  âœ… ä¿å­˜æœ€ä½³æ¨¡å‹ (Makespan: {best_avg_makespan:.2f})")
            else:
                no_improve_count += 1

        # å®šæœŸä¿å­˜æ£€æŸ¥ç‚¹
        if episode % TRAINING_CONFIG['save_frequency'] == 0 and episode > 0:
            torch.save({
                'model_state_dict': ppo.policy.state_dict(),
                'episode': episode,
                'all_rewards': all_rewards[-100:],
                'all_makespans': all_makespans[-100:],
                'all_balances': all_balances[-100:]
            }, f'{model_dir}/checkpoint_ep{episode}.pt')

        # æ—©åœæ£€æŸ¥
        if no_improve_count >= TRAINING_CONFIG['early_stop_patience']:
            print(f"\nâš ï¸ æ—©åœ: è¿ç»­{no_improve_count}æ¬¡è¯„ä¼°æ— æ”¹è¿›")
            break

    pbar.close()

    # è®­ç»ƒç»“æŸ
    print("\n" + "=" * 60)
    print("âœ… è®­ç»ƒå®Œæˆ!")
    print(f"æœ€ä½³å¹³å‡å¥–åŠ±: {best_avg_reward:.2f}")
    print(f"æœ€ä½³å¹³å‡Makespan: {best_avg_makespan:.2f}")
    print(f"æœ€ä½³è´Ÿè½½å‡è¡¡åº¦: {best_balance:.2f}")
    print("=" * 60)

    # ç”Ÿæˆæœ€ç»ˆç”˜ç‰¹å›¾
    print("\nç”Ÿæˆæœ€ç»ˆè°ƒåº¦ç”˜ç‰¹å›¾...")
    try:
        # åŠ è½½æœ€ä½³æ¨¡å‹
        checkpoint = torch.load(f'{model_dir}/best_model.pt')
        ppo.policy.load_state_dict(checkpoint['model_state_dict'])

        # è¿è¡Œä¸€ä¸ªå®Œæ•´çš„episode
        tasks, usvs = train_instances[0]
        state = env.reset_with_instances(tasks, usvs)
        done = False
        steps = 0

        while not done and steps < num_tasks:
            usv_feats = state['usv_features']
            task_feats = state['task_features']
            distances = calculate_usv_task_distances(usv_feats[:, :2], task_feats[:, :2])
            graph = build_heterogeneous_graph(
                usv_feats, task_feats, distances,
                eta=TRAINING_CONFIG['eta'], device=device
            )
            graph.action_mask = state['action_mask']

            with torch.no_grad():
                action, _, _ = ppo.policy_old.act(graph, device, state['action_mask'])

            state, _, done, _ = env.step(action)
            steps += 1

        # ç”Ÿæˆç”˜ç‰¹å›¾
        generate_gantt_chart(env, save_path=f'{model_dir}/final_gantt_optimized.png')

        # æ‰“å°æœ€ç»ˆè¯„ä¼°æŠ¥å‘Š
        eval_result = evaluate_scheduling_result(env)
        print_evaluation_report(eval_result, "Optimized PPO Model")

    except Exception as e:
        print(f"ç”Ÿæˆç”˜ç‰¹å›¾æ—¶å‡ºé”™: {e}")

    # å…³é—­èµ„æº
    writer.close()

    print("\nè®­ç»ƒè„šæœ¬æ‰§è¡Œå®Œæ¯•ï¼")


if __name__ == "__main__":
    main()